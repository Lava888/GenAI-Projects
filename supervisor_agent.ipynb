{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\local_ZSACIIH\\Temp\\20\\ipykernel_40772\\214472991.py:32: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceBgeEmbeddings(\n",
      "c:\\Users\\ZSACIIH\\genaispace\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\local_ZSACIIH\\Temp\\20\\ipykernel_40772\\214472991.py:38: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  tcsdb = Chroma(persist_directory=\"concepts\", embedding_function=embeddings)\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    " \n",
    "# Supervisor agent tools\n",
    "class ToTCSAssistant(BaseModel):\n",
    "    \"\"\"Transfers work to a specialized assistant to handle TCS related queries.\"\"\"\n",
    " \n",
    "    request: str = Field(\n",
    "        description=\"Any necessary followup questions the TCS assistant should clarify before proceeding.\"\n",
    "    )\n",
    " \n",
    "class ToWEBAssistant(BaseModel):\n",
    "    \"\"\"Transfers work to a specialized assistant to handle WEB and non TCS related queries.\"\"\"\n",
    " \n",
    "    request: str = Field(\n",
    "        description=\"Any necessary followup questions the WEB assistant should clarify before proceeding.\"\n",
    "    )\n",
    " \n",
    "supervisor_agent_tools = [ToTCSAssistant, ToWEBAssistant]\n",
    "supervisor_tool_names = [\"ToTCSAssistant\", \"ToWEBAssistant\"]\n",
    " \n",
    "\"\"\"\n",
    "Load RAG database\n",
    "\"\"\"\n",
    " \n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    " \n",
    "# Initialize embedding model\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    " \n",
    "tcsdb = Chroma(persist_directory=\"concepts\", embedding_function=embeddings)\n",
    "webdb = Chroma(persist_directory=\"webdocdb\", embedding_function=embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LLM-powered autonomous agents are AI-driven entities that use large language models to understand, generate, and respond to natural language inputs. These agents can operate autonomously, making decisions and performing tasks without constant human intervention. They leverage the capabilities of LLMs for complex understanding and interaction in various applications like customer service, virtual assistants, or content generation.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 108, 'prompt_tokens': 9, 'total_tokens': 117, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Qwen2.5-7b', 'system_fingerprint': 'eli', 'id': 'eli-chatcmpl-1752049943', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--5c1e4f7e-c078-4f31-b6fa-69eb87951955-0', usage_metadata={'input_tokens': 9, 'output_tokens': 108, 'total_tokens': 117, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Module containing a custom function to create a ChatOpenAI object.\n",
    "\"\"\"\n",
    "from langchain_openai import ChatOpenAI\n",
    "from openai import OpenAI\n",
    "import httpx\n",
    "\n",
    "\n",
    "def get_eli_chat_model(temperature: float = 0.0, model_name: str = \"qwen2.5-7b\") -> ChatOpenAI:\n",
    "    \"\"\"\n",
    "    Create a ChatOpenAI instance, with the specified temperature and using the specified model.\n",
    "\n",
    "    Args:\n",
    "        temperature (float, optional): The temperature value to use. Defaults to 0.0.\n",
    "        model_name (str, optional): The name of the model to use. Defaults to \"qwen2.5-7b\".\n",
    "\n",
    "    Returns:\n",
    "        ChatOpenAI: An instance of the ChatOpenAI class.\n",
    "\n",
    "    Raises:\n",
    "        OpenAPIError: If there is an error with the API request.\n",
    "    \"\"\"\n",
    "def get_eli_chat_model(temperature: float = 0.0, model_name: str = \"qwen2.5-7b\"):\n",
    "    # Create an instance of the OpenAI client\n",
    "    client = OpenAI(\n",
    "        api_key=\"eli-fe3e07a3-8b92-493b-82e4-4500b80a562c\",\n",
    "        base_url=\"https://gateway.eli.gaia.gic.ericsson.se/api/openai/v1\",\n",
    "        http_client=httpx.Client(verify=False),\n",
    "    )\n",
    "    # Create an instance of ChatOpenAI\n",
    "    llm = ChatOpenAI(\n",
    "        model=model_name,\n",
    "        temperature=0,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "        api_key=\"eli-fe3e07a3-8b92-493b-82e4-4500b80a562c\",\n",
    "        base_url=\"https://gateway.eli.gaia.gic.ericsson.se/api/openai/v1\",\n",
    "    )\n",
    "    # Now we plug the OpenAI client into our langchain-openai interface\n",
    "    llm.client = client.chat.completions\n",
    "    return llm\n",
    "\n",
    "llm = get_eli_chat_model()\n",
    "llm.invoke(\"What is LLM powered automooous agent?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from typing import Annotated\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    " \n",
    "class AgentState(MessagesState):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompt:\n",
    "    def __init__(self, name: str, prompt: str) -> None:\n",
    "        self.name = name\n",
    "        self.__prompt = prompt\n",
    " \n",
    "    @property\n",
    "    def prompt(self) -> str:\n",
    "        return self.__prompt\n",
    " \n",
    "    def __str__(self) -> str:\n",
    "        return self.prompt\n",
    " \n",
    "    def __repr__(self) -> str:\n",
    "        return self.__str__()\n",
    "   \n",
    "# --- JIRA Agent ---\n",
    "__SUPERVISOR_AGENT = \"\"\"\n",
    "You are a supervisor assistant that delegates the tasks to the TCS and WEB agents based on the given input.\n",
    "\"\"\"\n",
    " \n",
    "SUPERVISOR_AGENT_PROMPT = Prompt(\n",
    "    name=\"supervisor_agent_prompt\",\n",
    "    prompt=__SUPERVISOR_AGENT,\n",
    ")\n",
    "\n",
    "__TCS_AGENT = \"\"\" You are a helpful TCS assistant that is specialized in answering queries based on TCS RAG database.\n",
    "Based on the user query ansewer the user query with the correct answer. Use provided tools to answer the user query.\n",
    "\"\"\"\n",
    "TCS_AGENT_PROMPT = Prompt(\n",
    "    name=\"tcs_agent_prompt\",\n",
    "    prompt=__TCS_AGENT,\n",
    ")\n",
    " \n",
    " \n",
    "__WEB_AGENT = \"\"\" You are a helpful assistant that is specialized in answering queries based on Web data RAG database.\n",
    "Based on the user query answer the user query with the correct answer. Use provided tools to fetch and answer the user query.\n",
    "\"\"\"\n",
    "WEB_AGENT_PROMPT = Prompt(\n",
    "    name=\"web_agent_prompt\",\n",
    "    prompt=__WEB_AGENT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def search_tcs_rag(query: str) -> str:\n",
    "    \"\"\"Searches the Knowledgebase for the given query and returns the result.\"\"\"\n",
    "    results = tcsdb.similarity_search_with_score(query, k=3)  # Search top 3 docs (adjust as needed)\n",
    "   \n",
    "    for doc, score in results:\n",
    "        result_doc = doc.page_content.split(\":\")[0].strip()\n",
    "    return result_doc\n",
    "tcs_agent_tools = [search_tcs_rag]\n",
    "tcs_agent_tool_names = [\"search_tcs_rag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def search_web_rag(query: str) -> str:\n",
    "    \"\"\"Searches the Knowledgebase for the given query and returns the result.\"\"\"\n",
    "    results = webdb.similarity_search_with_score(query, k=3)  # Search top 3 docs (adjust as needed)\n",
    "   \n",
    "    for doc, score in results:\n",
    "        result_doc = doc.page_content.split(\":\")[0].strip()\n",
    "    return result_doc\n",
    "web_agent_tools = [search_web_rag]\n",
    "web_agent_tool_names = [\"search_web_rag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate, MessagesPlaceholder\n",
    " \n",
    "def get_supervisor_agent_chain():\n",
    "    model = get_eli_chat_model()\n",
    "    model = model.bind_tools(supervisor_agent_tools)\n",
    "    system_message = SUPERVISOR_AGENT_PROMPT\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_message.prompt),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ],\n",
    "        template_format=\"jinja2\",\n",
    "    )\n",
    " \n",
    "    return prompt | model\n",
    " \n",
    "# --- TCS Agent ---\n",
    "def get_tcs_agent_chain():\n",
    "    model = get_eli_chat_model()\n",
    "    model = model.bind_tools(tcs_agent_tools)\n",
    "    system_message = TCS_AGENT_PROMPT\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_message.prompt),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ],\n",
    "        template_format=\"jinja2\",\n",
    "    )\n",
    " \n",
    "    return prompt | model\n",
    " \n",
    "# --- WEB Agent ---\n",
    "def get_web_agent_chain():\n",
    "    model = get_eli_chat_model()\n",
    "    model = model.bind_tools(web_agent_tools)\n",
    "    system_message = WEB_AGENT_PROMPT\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_message.prompt),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ],\n",
    "        template_format=\"jinja2\",\n",
    "    )\n",
    " \n",
    "    return prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.graph import END\n",
    " \n",
    "def route_supervisor_agent(state: AgentState,):\n",
    "    route = tools_condition(state)\n",
    "    print(\"route_supervisor_agent:\", route)\n",
    "    if route == END:\n",
    "        return END\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    if tool_calls:\n",
    "        if tool_calls[0][\"name\"] == \"ToTCSAssistant\":\n",
    "            return \"enter_tcs_agent\"\n",
    "        elif tool_calls[0][\"name\"] == \"ToWEBAssistant\":\n",
    "            return \"enter_web_agent\"\n",
    "        return \"supervisor_agent_tools\"\n",
    "    raise ValueError(\"Invalid route\")\n",
    " \n",
    "# --- TCS Agent ---\n",
    "def route_tcs_agent(state: AgentState,):\n",
    "    route = tools_condition(state)\n",
    "    print(\"route_tcs_agent:\", route)\n",
    "    if route == END:\n",
    "        return END\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    if tool_calls:\n",
    "        if tool_calls[0][\"name\"] in tcs_agent_tool_names:\n",
    "            return \"tcs_agent_tools\"\n",
    "    raise ValueError(\"Invalid route\")\n",
    " \n",
    "# --- WEB Agent ---\n",
    "def route_web_agent(state: AgentState,):\n",
    "    route = tools_condition(state)\n",
    "    print(\"route_web_agent:\", route)\n",
    "    if route == END:\n",
    "        return END\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    if tool_calls:\n",
    "        if tool_calls[0][\"name\"] in web_agent_tool_names:\n",
    "            return \"web_agent_tools\"\n",
    "    raise ValueError(\"Invalid route\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.messages import ToolMessage\n",
    " \n",
    "def handle_tool_error(state) -> dict:\n",
    "    error = state.get(\"error\")\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    return {\n",
    "        \"messages\": [\n",
    "        ToolMessage(\n",
    "            content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n",
    "            tool_call_id=tc[\"id\"],\n",
    "            )\n",
    "            for tc in tool_calls\n",
    "        ]\n",
    "    }\n",
    " \n",
    "#---Supervisor tools node---\n",
    "def supervisor_tool_node_with_fallback(_) -> dict:\n",
    "        return ToolNode(supervisor_agent_tools).with_fallbacks(\n",
    "            [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n",
    "    )\n",
    " \n",
    "#---TCS tools node---\n",
    "def tcs_tool_node_with_fallback(_) -> dict:\n",
    "        return ToolNode(tcs_agent_tools).with_fallbacks(\n",
    "            [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n",
    "    )\n",
    " \n",
    "#---WEB tools node---\n",
    "def web_tool_node_with_fallback(_) -> dict:\n",
    "        return ToolNode(web_agent_tools).with_fallbacks(\n",
    "            [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n",
    "    )\n",
    " \n",
    " \n",
    "#---Supervisor node---\n",
    "def supervisor_node(state: AgentState):\n",
    "    supervisor_chain = get_supervisor_agent_chain()\n",
    " \n",
    "    while True:\n",
    "        response = supervisor_chain.invoke(state)\n",
    "        print(\"jira_node:\", response)\n",
    "        if not response.tool_calls and (\n",
    "            not response.content\n",
    "            or isinstance(response.content, list)\n",
    "            and not response.content[0].get(\"text\")\n",
    "        ):\n",
    "            messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "            state = {**state, \"messages\": messages}\n",
    "        else:\n",
    "            break\n",
    " \n",
    "    return {\"messages\": response}\n",
    " \n",
    "#---TCS node---\n",
    "def tcs_node(state: AgentState):\n",
    "    tcs_chain = get_tcs_agent_chain()\n",
    " \n",
    "    while True:\n",
    "        response = tcs_chain.invoke(state)\n",
    "        print(\"tcs_node:\", response)\n",
    "        if not response.tool_calls and (\n",
    "            not response.content\n",
    "            or isinstance(response.content, list)\n",
    "            and not response.content[0].get(\"text\")\n",
    "        ):\n",
    "            messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "            state = {**state, \"messages\": messages}\n",
    "        else:\n",
    "            break\n",
    " \n",
    "    return {\"messages\": response}\n",
    " \n",
    "#---WEB node---\n",
    "def web_node(state: AgentState):\n",
    "    web_chain = get_web_agent_chain()\n",
    " \n",
    "    while True:\n",
    "        response = web_chain.invoke(state)\n",
    "        print(\"web_node:\", response)\n",
    "        if not response.tool_calls and (\n",
    "            not response.content\n",
    "            or isinstance(response.content, list)\n",
    "            and not response.content[0].get(\"text\")\n",
    "        ):\n",
    "            messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "            state = {**state, \"messages\": messages}\n",
    "        else:\n",
    "            break\n",
    " \n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
