from langchain_openai import ChatOpenAI
import httpx
from openai import OpenAI
 
def get_eli_chat_model(temperature: float = 0.0, model_name: str = "qwen2.5-7b"):
    # Create an instance of the OpenAI client
    client = OpenAI(
        api_key="eli-f801b435-a683-4b55-b0a6-e1320175d9f8",
        base_url="https://gateway.eli.gaia.gic.ericsson.se/api/openai/v1",
        http_client=httpx.Client(verify=False),
    )
    # Create an instance of ChatOpenAI
    llm = ChatOpenAI(
        model=model_name,
        temperature=temperature,
        max_tokens=None,
        timeout=None,
        max_retries=2,
        api_key="eli-f801b435-a683-4b55-b0a6-e1320175d9f8",
        base_url="https://gateway.eli.gaia.gic.ericsson.se/api/openai/v1",
    )
    # Now we plug the OpenAI client into our langchain-openai interface
    llm.client = client.chat.completions
    return llm
 
 
pip install wikipedia
 
pip install wikipedia
 
from langchain import hub
from langchain.agents import AgentExecutor, create_structured_chat_agent
from langchain.memory import ConversationBufferMemory
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.tools import Tool
from langchain_openai import ChatOpenAI
 
 
# Define Tools
def get_current_time(query):
    """Returns the current time in H:MM AM/PM format."""
    import datetime
 
    now = datetime.datetime.now()
    return now.strftime("%I:%M %p")
 
 
def search_wikipedia(query):
    """Searches Wikipedia and returns the summary of the first result."""
    from wikipedia import summary
 
    try:
        # Limit to two sentences for brevity
        return summary(query, sentences=2)
    except:
        return "I couldn't find any information on that."
 
 
# Define the tools that the agent can use
tools = [
    Tool(
        name="Time",
        func=get_current_time,
        description="Useful for when you need to know the current time.",
    ),
    Tool(
        name="Wikipedia",
        func=search_wikipedia,
        description="Useful for when you need to know information about a topic.",
    ),
]
 
# Load the correct JSON Chat Prompt from the hub
prompt = hub.pull("hwchase17/structured-chat-agent")
 
# Initialize a ChatOpenAI model
llm = get_eli_chat_model()
 
# Create a structured Chat Agent with Conversation Buffer Memory
# ConversationBufferMemory stores the conversation history, allowing the agent to maintain context across interactions
memory = ConversationBufferMemory(
    memory_key="chat_history", return_messages=True)
 
# create_structured_chat_agent initializes a chat agent designed to interact using a structured prompt and tools
# It combines the language model (llm), tools, and prompt to create an interactive agent
agent = create_structured_chat_agent(llm=llm, tools=tools, prompt=prompt)
 
# AgentExecutor is responsible for managing the interaction between the user input, the agent, and the tools
# It also handles memory to ensure context is maintained throughout the conversation
agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent,
    tools=tools,
    verbose=True,
    memory=memory,  # Use the conversation memory to maintain context
    handle_parsing_errors=True,  # Handle any parsing errors gracefully
)
 
# Initial system message to set the context for the chat
# SystemMessage is used to define a message from the system to the agent, setting initial instructions or context
initial_message = "You are an AI assistant that can provide helpful answers using available tools.\nIf you are unable to answer, you can use the following tools: Time and Wikipedia."
memory.chat_memory.add_message(SystemMessage(content=initial_message))
 
# Chat Loop to interact with the user
while True:
    user_input = input("User: ")
    if user_input.lower() == "exit":
        break
 
    # Add the user's message to the conversation memory
    memory.chat_memory.add_message(HumanMessage(content=user_input))
 
    # Invoke the agent with the user input and the current chat history
    response = agent_executor.invoke({"input": user_input})
    print("Bot:", response["output"])
 
    # Add the agent's response to the conversation memory
    memory.chat_memory.add_message(AIMessage(content=response["output"]))
 
 ===========================================================
 
 # Docs: https://python.langchain.com/v0.1/docs/modules/tools/custom_tools/
 
# Import necessary libraries
from langchain import hub
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain.pydantic_v1 import BaseModel, Field
from langchain_core.tools import StructuredTool, Tool
from langchain_openai import ChatOpenAI
from langchain.tools import tool
 
import httpx
from openai import OpenAI
 
def get_eli_chat_model(temperature: float = 0.0, model_name: str = "qwen2.5-7b"):
    # Create an instance of the OpenAI client
    client = OpenAI(
        api_key="eli-18edf06d-cab1-414c-b7f0-623c60eb291b",
        base_url="https://gateway.eli.gaia.gic.ericsson.se/api/openai/v1",
        http_client=httpx.Client(verify=False),
    )
    # Create an instance of ChatOpenAI
    llm = ChatOpenAI(
        model=model_name,
        temperature=temperature,
        max_tokens=None,
        timeout=None,
        max_retries=2,
        api_key="eli-18edf06d-cab1-414c-b7f0-623c60eb291b",
        base_url="https://gateway.eli.gaia.gic.ericsson.se/api/openai/v1",
    )
    # Now we plug the OpenAI client into our langchain-openai interface
    llm.client = client.chat.completions
    return llm
 
llm = get_eli_chat_model()
 
# setup the simple tools using LangChain tool decorator
@tool
def add(a: int, b: int) -> int:
    """Add two numbers."""
    return a + b
 
 
@tool
def multiply(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b
 
 
@tool
def square(a: int) -> int:
    """Calculates the square of a number."""
    a = int(a)
    return a * a
 
# setup the toolkit
toolkit = [add, multiply, square]

================================================

from langchain.agents import create_tool_calling_agent
 
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
 
#define system prompt for tool calling agent
system_prompt = """ You are a mathematical assistant.
        Use your tools to answer questions. If you do not have a tool to
        answer the question, say so. """
 
tool_calling_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        MessagesPlaceholder("chat_history", optional=True),
        ("human", "{input}"),
        MessagesPlaceholder("agent_scratchpad"),
    ]
)
 
tool_runnable = create_tool_calling_agent(llm, toolkit, prompt  = tool_calling_prompt)
tool_actions = tool_runnable.invoke({"input": "hi! what's 1+1 and  2 times 2", 'intermediate_steps': []})

==============================================================

from typing import TypedDict, Annotated,Union
from langchain_core.agents import AgentAction, AgentFinish
from langchain.agents.output_parsers.tools import ToolAgentAction
from langchain_core.messages import BaseMessage
import operator
 
 
class AgentState(TypedDict):
   # The input string from human
   input: str
   # The list of previous messages in the conversation
   chat_history: list[BaseMessage]
   # The outcome of a given call to the agent
   # Needs 'list' as a valid type as the tool agent returns a list.
   # Needs `None` as a valid type, since this is what this will start as
   # this state will be overwritten with the latest everytime the agent is run
   agent_outcome: Union[AgentAction, list, ToolAgentAction, AgentFinish, None]
 
   # List of actions and corresponding observations
   # These actions should be added onto the existing so we use `operator.add`
   # to append to the list of past intermediate steps
   intermediate_steps: Annotated[list[Union[tuple[AgentAction, str], tuple[ToolAgentAction, str]]], operator.add]
 
def run_tool_agent(state):
    agent_outcome = tool_runnable.invoke(state)
 
    #this agent will overwrite the agent outcome state variable
    return {"agent_outcome": agent_outcome}
 
from langgraph.prebuilt import ToolNode
 
# tool executor invokes the tool action specified from the agent runnable
# they will become the nodes that will be called when the agent decides on a tool action.
 
tool_executor = ToolNode(toolkit)
 
# Define the function to execute tools
# This node will run a different tool as specified in the state variable agent_outcome
def execute_tools(state):
    # Get the most recent agent_outcome - this is the key added in the `agent` above
    agent_action = state['agent_outcome']
    if type(agent_action) is not list:
        agent_action = [agent_action]
    steps = []
    #sca only returns an action while tool calling returns a list
    # convert single actions to a list
   
    for action in agent_action:
    # Execute the tool
        output = tool_executor.invoke(action)
        print(f"The agent action is {action}")
        print(f"The tool result is: {output}")
        steps.append((action, str(output)))
    # Return the output
    return {"intermediate_steps": steps}
 
def should_continue(data):
    # If the agent outcome is an AgentFinish, then we return `exit` string
    # This will be used when setting up the graph to define the flow
    if isinstance(data['agent_outcome'], AgentFinish):
        return "END"
    # Otherwise, an AgentAction is returned
    # Here we return `continue` string
    # This will be used when setting up the graph to define the flow
    else:
        return "CONTINUE"
 
from langgraph.graph import  START, END, StateGraph
 
# Define a new graph
workflow = StateGraph(AgentState)
 
# Add the nodes
# When nodes are called, the functions for to the tools will be called.
workflow.add_node("maths_agent", run_agent)
# Add tool invocation node to the graph
workflow.add_node("action", execute_tools)
 
#workflow.add_edge("START", "maths_agent")
 
 
# Define which node the graph will invoke at start.
workflow.set_entry_point("maths_agent")
 
 
# Add flow logic with static edge.
# Each time a tool is invoked and completed we want to
# return the result to the agent to assess if task is complete or to take further actions
 
#each action invocation has an edge leading to the agent node.
workflow.add_edge('action', 'maths_agent')
 
# Add flow logic with conditional edge.
workflow.add_conditional_edges(
    # first parameter is the starting node for the edge
    "maths_agent",
    # the second parameter specifies the logic function to be run
    # to determine which node the edge will point to given the state.
    should_continue,
 
    #third parameter defines the mapping between the logic function
    #output and the nodes on the graph
    # For each possible output of the logic function there must be a valid node.
    {
        # If 'continue' we proceed to the action node.
        "CONTINUE": "action",
        # Otherwise we end invocations with the END node.
        "END": END
    }
)
 
 
 
================================================================

from langgraph.checkpoint.memory import MemorySaver
 
memory = MemorySaver()
 
# Finally, compile the graph!
# This compiles it into a LangChain Runnable,
app = workflow.compile(checkpointer = memory)
 
inputs = {"input": "give me 1+1 and then 2 times 2", "chat_history": []}
config = {"configurable": {"thread_id": "1"}}
 
for s in app.stream(inputs, config = config):
    print(list(s.values())[0])
    print("----")
 
from dotenv import load_dotenv
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnableParallel, RunnableLambda
from langchain_openai import ChatOpenAI
import httpx
from openai import OpenAI
# Load environment variables from .env
 
 
# Create a ChatOpenAI model
def get_eli_chat_model(temperature: float = 0.0, model_name: str = "qwen2.5-7b"):
    # Create an instance of the OpenAI client
    client = OpenAI(
        api_key="eli-18edf06d-cab1-414c-b7f0-623c60eb291b",
        base_url="https://gateway.eli.gaia.gic.ericsson.se/api/openai/v1",
        http_client=httpx.Client(verify=False),
    )
    # Create an instance of ChatOpenAI
    llm = ChatOpenAI(
        model=model_name,
        temperature=temperature,
        max_tokens=None,
        timeout=None,
        max_retries=2,
        api_key="eli-18edf06d-cab1-414c-b7f0-623c60eb291b",
        base_url="https://gateway.eli.gaia.gic.ericsson.se/api/openai/v1",
    )
    # Now we plug the OpenAI client into our langchain-openai interface
    llm.client = client.chat.completions
    return llm
 
model = get_eli_chat_model()
 
# Define prompt template
prompt_template = ChatPromptTemplate.from_messages(
    [
        ("system", "You are an expert product reviewer."),
        ("human", "List the main features of the product {product_name}."),
    ]
)
 
 
# Define pros analysis step
def analyze_pros(features):
    pros_template = ChatPromptTemplate.from_messages(
        [
            ("system", "You are an expert product reviewer."),
            (
                "human",
                "Given these features: {features}, list the pros of these features.",
            ),
        ]
    )
    return pros_template.format_prompt(features=features)
 
 
# Define cons analysis step
def analyze_cons(features):
    cons_template = ChatPromptTemplate.from_messages(
        [
            ("system", "You are an expert product reviewer."),
            (
                "human",
                "Given these features: {features}, list the cons of these features.",
            ),
        ]
    )
    return cons_template.format_prompt(features=features)
 
 
# Combine pros and cons into a final review
def combine_pros_cons(pros, cons):
    return f"Pros:\n{pros}\n\nCons:\n{cons}"
 
 
# Simplify branches with LCEL
pros_branch_chain = (
    RunnableLambda(lambda x: analyze_pros(x)) | model | StrOutputParser()
)
 
cons_branch_chain = (
    RunnableLambda(lambda x: analyze_cons(x)) | model | StrOutputParser()
)
 
# Create the combined chain using LangChain Expression Language (LCEL)
chain = (
    prompt_template
    | model
    | StrOutputParser()
    | RunnableParallel(branches={"pros": pros_branch_chain, "cons": cons_branch_chain})
    | RunnableLambda(lambda x: combine_pros_cons(x["branches"]["pros"], x["branches"]["cons"]))
)
 
# Run the chain
result = chain.invoke({"product_name": "MacBook Pro"})
 
# Output
print(result)
 
 
==============================================================

Hi all, 
I have written the code for Agricultural Bot with out db, working fine as of now (we can do with db also)
please look at this code and try to execute it 
 
from langdetect import detect
from deep_translator import GoogleTranslator
from indic_transliteration import sanscript
from indic_transliteration.sanscript import transliterate
from langchain_community.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from openai import OpenAI
import httpx
 
 
import streamlit as st
 
st.title("Agricultural BOT")
 
# Initialize LLM
def get_eli_chat_model(temperature: float = 0.0, model_name: str = "qwen2.5-7b"):
    # Create an instance of the OpenAI client
    client = OpenAI(
        api_key="eli-18edf06d-cab1-414c-b7f0-623c60eb291b",
        base_url="https://gateway.eli.gaia.gic.ericsson.se/api/openai/v1",
        http_client=httpx.Client(verify=False),
    )
    # Create an instance of ChatOpenAI
    llm = ChatOpenAI(
        model=model_name,
        temperature=temperature,
        max_tokens=None,
        timeout=None,
        max_retries=2,
        api_key="eli-18edf06d-cab1-414c-b7f0-623c60eb291b",
        base_url="https://gateway.eli.gaia.gic.ericsson.se/api/openai/v1",
    )
    # Now we plug the OpenAI client into our langchain-openai interface
    llm.client = client.chat.completions
    return llm
llm = get_eli_chat_model()
 
 
 
class MultilingualHelper:
    def __init__(self, llm):
        self.llm = llm
        self.translit_languages = {
            "te": sanscript.TELUGU,
            "hi": sanscript.DEVANAGARI,
            "ta": sanscript.TAMIL
        }
 
   
    # Language Detection
    def detect_language(self, text: str) -> str:
        """Detect language code using langdetect"""
        try:
            lang_code = detect(text)
        except:
            lang_code = "en"
        return lang_code
 
 
    # Transliterate if needed
    def transliterate_if_needed(self, text: str, lang: str) -> str:
        """Transliterate Latin letters to native script if needed"""
        if any('a' <= c.lower() <= 'z' for c in text) and lang in self.translit_languages:
            try:
                text = transliterate(text, sanscript.ITRANS, self.translit_languages[lang])
            except:
                pass
        return text
 
 
    # Translate text
    def translate_text(self, text: str, target_lang: str = "en") -> str:
        """Translate text to target language using deep_translator"""
        try:
            return GoogleTranslator(source='auto', target=target_lang).translate(text)
        except Exception as e:
            print("Translation error:", e)
            return text
 
 
    # Call LLM for answer
    def get_llm_answer(self, english_question: str) -> str:
        """Call LLM to get answer in English"""
        prompt = ChatPromptTemplate.from_template(
            """Answer the following question in detail:\n{question}
            give the proper answers based on the question {question} not include extra points
            answer format is bold index and point-wise with detailed explanation don't mention name as point in the output
            give suggestion to user based on the result
            """
        )
        return self.llm.predict(prompt.format(question=english_question))
 
 
 
#Full multilingual QA
def qa_multilingual(user_query: str, helper: MultilingualHelper) -> str:
    # Detect user language
    source_lang = helper.detect_language(user_query)
 
    print(f"Detected language: {source_lang}")
 
    # Transliterate if needed
    if source_lang in helper.translit_languages:
        user_query = helper.transliterate_if_needed(user_query, source_lang)
 
    # Translate question to English
    english_question = helper.translate_text(user_query, target_lang="en") if source_lang != "en" else user_query
 
    # Get LLM answer in English
    english_answer = helper.get_llm_answer(english_question)
 
    # Translate answer back to user's language
    final_answer = helper.translate_text(english_answer, target_lang=source_lang) if source_lang != "en" else english_answer
 
    return final_answer
 
 
# ask the queastion here
user_query = st.chat_input("Ask your question here in any language")
helper = MultilingualHelper(llm)
 
if user_query:  
    st.write("Question: ", user_query)
    with st.spinner("Generating answer..."):
        answer = qa_multilingual(user_query,helper)
    print(answer)
    st.write(answer)
 
=============================================

from langchain_openai import ChatOpenAI
from openai import OpenAI
import httpx
 
# Initialize LLM
def get_eli_chat_model(temperature: float = 0.0, model_name: str = "qwen2.5-7b"):
    # Create an instance of the OpenAI client
    client = OpenAI(
        api_key="eli-18edf06d-cab1-414c-b7f0-623c60eb291b",
        base_url="https://gateway.eli.gaia.gic.ericsson.se/api/openai/v1",
        http_client=httpx.Client(verify=False),
    )
    # Create an instance of ChatOpenAI
    llm = ChatOpenAI(
        model=model_name,
        temperature=temperature,
        max_tokens=None,
        timeout=None,
        max_retries=2,
        api_key="eli-18edf06d-cab1-414c-b7f0-623c60eb291b",
        base_url="https://gateway.eli.gaia.gic.ericsson.se/api/openai/v1",
    )
    # Now we plug the OpenAI client into our langchain-openai interface
    llm.client = client.chat.completions
    return llm
llm = get_eli_chat_model()
 
 
 
llm = get_eli_chat_model()
from langchain_core.messages import (
    BaseMessage,
    HumanMessage,
)
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langgraph.graph import END, StateGraph, START
 
 
def create_agent(llm, system_message: str):
    """Create an agent."""
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                " You are a helpful AI assistant, collaborating with other assistants."
                " If you are unable to fully answer, that's OK, another assistant "
                " will help where you left off. Execute what you can to make progress."
                " Remember to focus on your given task only, do not do another assistant work ."
                " If you or any of the other assistants have the final answer or deliverable,"
                " prefix your response with FINAL ANSWER so the team knows to stop."
                " \n{system_message}",
            ),
            MessagesPlaceholder(variable_name="messages"),
        ]
    )
    prompt = prompt.partial(system_message=system_message)
    return prompt | llm
 
llm = get_eli_chat_model()
from langchain_core.messages import (
    BaseMessage,
    HumanMessage,
)
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langgraph.graph import END, StateGraph, START
 
 
def create_agent(llm, system_message: str):
    """Create an agent."""
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                " You are a helpful AI assistant, collaborating with other assistants."
                " If you are unable to fully answer, that's OK, another assistant "
                " will help where you left off. Execute what you can to make progress."
                " Remember to focus on your given task only, do not do another assistant work ."
                " If you or any of the other assistants have the final answer or deliverable,"
                " prefix your response with FINAL ANSWER so the team knows to stop."
                " \n{system_message}",
            ),
            MessagesPlaceholder(variable_name="messages"),
        ]
    )
    prompt = prompt.partial(system_message=system_message)
    return prompt | llm
 
import operator
from typing import Annotated, Sequence, TypedDict, List
 
 
class AgentState(TypedDict):
    sender: str
    messages: Annotated[Sequence[BaseMessage], operator.add]
 
import functools
 
def agent_node(state, agent, name):
    result = agent.invoke(state)
    return {
        "messages": [result],
        "sender": name,
    }
 
# Test Scenarios Writer and Reviewer Agents
test_scenario_writer_agent = create_agent(
    llm=llm,
    system_message="""
        You must provide a set of coherent and well defined test scenarios for another agent to write codes,
        or improve existant test scenarions following given instructions.
        You must not provide codes or other functions that are not you primary objective.
    """
)
ts_writer_node = functools.partial(agent_node, agent=test_scenario_writer_agent, name="ScenarioWriter")
 
test_scenario_reviewer_agent = create_agent(
    llm,
    system_message="""
        You must review a set of test scenarios,
        and if the test scenarios needs enhancement you must prefix your response with REWRITE and provide instructions on how to improve.
        You must not provide codes or other functions that are not you primary objective.
    """,
)
ts_reviewer_node = functools.partial(agent_node, agent=test_scenario_reviewer_agent, name="ScenarioReviewer")
 
# Test Code Writer and Reviewer
test_code_writer_agent = create_agent(
    llm=llm,
    system_message=f"You must write python code for test scenarios",
)
tc_writer_node = functools.partial(agent_node, agent=test_code_writer_agent, name="CodeWriter")
 
test_code_reviewer_agent = create_agent(
    llm=llm,
    system_message=f"""
        You must review python code,
        and if the code needs enhancement you must prefix your response with REWRITE and provide instructions on how to improve.
        You must not provide codes or other functions that are not you primary objective.
    """,
)
tc_reviewer_node = functools.partial(agent_node, agent=test_code_reviewer_agent, name="CodeReviewer")
 
from typing import Literal
 
 
def router(state) -> Literal["__end__", "continue", "rewrite"]:
    # This is the router
    messages = state["messages"]
    last_message = messages[-1]
    if "REWRITE" in last_message.content:
        return "rewrite"
    if "FINAL ANSWER" in last_message.content:
        # Any agent decided the work is done
        return "__end__"
    return "continue"
 
# Create Workflow
workflow = StateGraph(AgentState)
 
workflow.add_node("ScenarioWriter", ts_writer_node)
workflow.add_node("ScenarioReviewer", ts_reviewer_node)
workflow.add_node("CodeWriter", tc_writer_node)
workflow.add_node("CodeReviewer", tc_reviewer_node)
 
workflow.add_conditional_edges(
    "ScenarioWriter",
    router,
    {
        "continue": "ScenarioReviewer",
        "__end__": END
    },
)
 
workflow.add_conditional_edges(
    "ScenarioReviewer",
    router,
    {
        "continue": "CodeWriter",
        "rewrite": "ScenarioWriter",
        "__end__": "CodeWriter"
    },
)
 
workflow.add_conditional_edges(
    "CodeWriter",
    router,
    {
        "continue": "CodeReviewer",
        "__end__": END
    },
)
 
workflow.add_conditional_edges(
    "CodeReviewer",
    router,
    {
        "continue": "ScenarioWriter",
        "rewrite": "CodeWriter",
        "__end__": END
    },
)
 
 
workflow.add_edge(START, "ScenarioWriter")
graph = workflow.compile()
 
objective = """
    As a user, I want to securely log in to the application using my email and password, so I can access my personalized dashboard and features.
    The system should also support role-based access control to restrict access to specific areas of the application based on user roles (e.g., admin, user, guest)."
"""
 
events = graph.stream(
    {
        "messages": [
            HumanMessage(
                content=f" I would like for you to write 3 test scenarios, for this this objective : {objective}"
                f" for each test scenario I would like a python code script."
                " Once you code it up, finish."
            )
        ],
    },
    # Maximum number of steps to take in the graph
    {"recursion_limit": 150},
)
 
for s in events:
    print("#"*50)
    for k,v  in s.items():
        print(f"{k} : \n", s[k]['messages'][-1].content)
    print()
 
 
 ============================================
 
 
 from flask import Flask, render_template, request, jsonify
import operator
from datetime import datetime
from typing import Annotated, TypedDict, Union, Literal, Optional
from langchain.agents import create_react_agent, AgentExecutor
from langchain_community.chat_models import ChatOllama
from langchain_core.agents import AgentAction, AgentFinish
from langchain_core.messages import BaseMessage
from langchain_core.tools import tool
from langgraph.graph import END, StateGraph
from langgraph.prebuilt import ToolExecutor, ToolInvocation
from langchain.prompts import PromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_chroma import Chroma
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain.retrievers.document_compressors import EmbeddingsFilter
from langchain_core.pydantic_v1 import BaseModel, Field
from langgraph.checkpoint.sqlite import SqliteSaver
from langchain_core.runnables import RunnableConfig
from os.path import exists
import uuid
import time
import threading
from threading import Lock
import paramiko
import re
import requests
import os
import numpy as np
 
 
nc = {"bsc":"bsc,localhost,asif,rootroot","msc":"msc,localhost,asif,rootroot","hlr":"hlr,localhost,asif,rootroot"}
 
import operator
from datetime import datetime
from typing import Annotated, TypedDict, Union, Literal, Optional
from langchain.agents import create_react_agent
 
 
=========================================================

from openai import OpenAI
 
from langchain_openai import ChatOpenAI
from openai import OpenAI
import httpx
 
 
import httpx
 
 ===========================================
 
 import operator
from datetime import datetime
from typing import Annotated, TypedDict, Union, Literal, Optional
from langchain.agents import create_react_agent
from langchain_core.agents import AgentAction, AgentFinish
from langchain_core.messages import BaseMessage
from langchain_core.tools import tool
from langgraph.graph import END, StateGraph
from langgraph.prebuilt import ToolNode
from langchain.prompts import PromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings
import paramiko
 
 
import operator
from datetime import datetime
from typing import Annotated, TypedDict, Union, Literal, Optional
from langchain.agents import create_react_agent
from langchain_core.agents import AgentAction, AgentFinish
from langchain_core.messages import BaseMessage
from langchain_core.tools import tool
from langgraph.graph import END, StateGraph
from langgraph.prebuilt import ToolNode
from langchain.prompts import PromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings
import paramiko
from langchain_openai import ChatOpenAI
from openai import OpenAI
import httpx
 
 
from langgraph.checkpoint import SqliteSaver
 
import faiss
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore
 
 
from langchain_community.vectorstores import FAISS
 
import streamlit as st
from PyPDF2 import PdfReader #library to read pdf files
from langchain.text_splitter import RecursiveCharacterTextSplitter#library to split pdf files
import os
from langchain.vectorstores import FAISS #for vector embeddings
from langchain.chains.question_answering import load_qa_chain #to chain the prompts
from langchain.prompts import PromptTemplate #to create prompt templates
from langchain import hub
from langchain_chroma import Chroma
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_openai import ChatOpenAI
import httpx
from openai import OpenAI
 
embeddings = HuggingFaceBgeEmbeddings(
            model_name="BAAI/bge-base-en-v1.5",
            model_kwargs={"device": "cpu", "trust_remote_code": True},
            encode_kwargs={"normalize_embeddings": True},)  
 
def get_eli_chat_model(temperature: float = 0.0, model_name: str = "qwen2.5-7b"):
    # Create an instance of the OpenAI client
    client = OpenAI(
        api_key="eli-b0bde4e3-cf64-4f1f-80e7-3bf0e4006478",
        base_url="https://gateway.eli.gaia.gic.ericsson.se/api/openai/v1",
        http_client=httpx.Client(verify=False),
    )
    # Create an instance of ChatOpenAI
    llm = ChatOpenAI(
        model=model_name,
        temperature=temperature,
        max_tokens=None,
        timeout=None,
        max_retries=2,
        api_key="eli-b0bde4e3-cf64-4f1f-80e7-3bf0e4006478",
        base_url="https://gateway.eli.gaia.gic.ericsson.se/api/openai/v1",
    )
    # Now we plug the OpenAI client into our langchain-openai interface
    llm.client = client.chat.completions
    return llm
 
def get_pdf_text(pdf_docs):
    text = ""
    # iterate over all pdf files uploaded
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        # iterate over all pages in a pdf
        for page in pdf_reader.pages:
            text += page.extract_text()
    return text
 
def get_text_chunks(text):
    # create an object of RecursiveCharacterTextSplitter with specific chunk size and overlap size
    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 10000, chunk_overlap = 1000)
    # now split the text we have using object created
    chunks = text_splitter.split_text(text)
 
    return chunks
 
def get_vector_store(text_chunks):
    # Create an embedding instance.  
    vector_store = FAISS.from_texts(text_chunks,embeddings) # use the embedding object on the splitted text of pdf docs
    vector_store.save_local("faiss_index") # save the embeddings in local
 
def get_conversation_chain():
 
    model = get_eli_chat_model()
    # define the prompt
    prompt_template = """
    Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in
    provided context just say, "answer is not available in the context", don't provide the wrong answer\n\n
    Context:\n {context}?\n
    Question: \n{question}\n
 
    Answer:
    """
 
   
    prompt = PromptTemplate(template = prompt_template, input_variables= ["context","question"])
    print("Prompt: ", prompt)
    chain = load_qa_chain(model,chain_type="stuff",prompt = prompt)
 
    return chain
 
def user_input(user_question):
    # user_question is the input question
    # load the local faiss db
    new_db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
 
    # using similarity search, get the answer based on the input
    docs = new_db.similarity_search(user_question)
    print("Documents Returned: ", docs)
    chain = get_conversation_chain()
 
   
    response = chain(
        {"input_documents":docs, "question": user_question}
        , return_only_outputs=True)
 
   
    return response["output_text"]
   
 
def main():
    st.set_page_config("Chat PDF")
    st.header("Chat with PDF using LLM")
    with st.sidebar:
        st.title("Menu:")
        pdf_docs = st.file_uploader("Upload your PDF Files and Click on the Submit & Process Button", accept_multiple_files=True)
        if st.button("Submit & Process"):
            with st.spinner("Processing..."):
                raw_text = get_pdf_text(pdf_docs)
                text_chunks = get_text_chunks(raw_text)
                get_vector_store(text_chunks)
                st.success("Done")
 
    query = st.chat_input("Ask a Question")
    if query:
        with st.chat_message("user"):
            st.write(query)
        with st.spinner("Generating response..."):
            response = user_input(query)
            with st.chat_message("assistant"):
                st.write(response)
 
if __name__ == "__main__":
    main()
 
 =============================================================
